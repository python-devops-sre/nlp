{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UMqBL77hMXP2"
      },
      "source": [
        "# Setting up close source and open source LLMs\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Mastering-NLP-from-Foundations-to-LLMs/blob/liors_branch/Chapter8_notebooks/Ch8_Setting_Up_Close_Source_and_Open_Source_LLMs.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K3nyaPpW4GVh"
      },
      "source": [
        "**The purpose of this notebook:**   \n",
        "In this notebook we demostrate how to set up LLMs for prompting via code.  \n",
        "We start by experimenting with OpenAI's GPT via API,  \n",
        "And then we import an open source free LLM and experiment with it locally.  \n",
        "\n",
        "\n",
        "**Requirements:**  \n",
        "* When running in Colab, use this runtime notebook setting: `Python 3, CPU`  \n",
        "* OpenAI's API is preferrable so to utilize all the notebooks functionalities  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g54Uf66Vz9Fi"
      },
      "source": [
        ">*```Disclaimer: The content and ideas presented in this notebook are solely those of the authors and do not represent the views or intellectual property of the authors' employers.```*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsuY22Ld4RLh"
      },
      "source": [
        "## Part 1 - Using LLMs in Python Using API\n",
        "In this part we cover the process of setting up access to OpenAI's `gpt-3.5-turbo` via their API.  \n",
        "We lay out the steps per the description in the book.  \n",
        "> **Note that an API key needs to be generated ahead of time via OpenAI's website, and is available for you once you set up an account.**  \n",
        " If you don't have a key, skip to part 2 for setting up open source LLMs for free.  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5vHoDoKTE4tt"
      },
      "source": [
        "Install:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV2EqicfE4tu",
        "outputId": "307656e0-c6b0-48cf-a652-7ce12805933b"
      },
      "outputs": [],
      "source": [
        "# REMARK:\n",
        "# If the below code error's out due to a Python package discrepency, it may be because new versions are causing it.\n",
        "# In which case, set \"default_installations\" to False to revert to the original image:\n",
        "default_installations = True\n",
        "if default_installations:\n",
        "    !pip -q install --upgrade openai\n",
        "else:\n",
        "    import requests\n",
        "    text_file_path = \"closed_open_LLM.txt\"\n",
        "    url = \"https://raw.githubusercontent.com/python-devops-sre/nlp/master/requirements/\" + text_file_path           \n",
        "    res = requests.get(url)\n",
        "    with open(text_file_path, \"w\") as f:\n",
        "      f.write(res.text)\n",
        "      \n",
        "    !pip install -r closed_open_LLM.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lKvVWqxUE4tv"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H0Y60k1fE4tv"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym-8iF3xE4tw"
      },
      "source": [
        "Define OpenAI's API key:  \n",
        "**You must provide a key and paste it as a string!**  \n",
        "If you don't have a key, skip to part 2 for setting up open source LLMs for free.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OLJdgPbFE4tw"
      },
      "outputs": [],
      "source": [
        "# paste your key here as a string: \"...\"\n",
        "api_key = \"...\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cmNWLxJ3E4tx"
      },
      "source": [
        "### Experiment with `gpt-3.5-turbo`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UeyM5927E4ty"
      },
      "source": [
        "Code Settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JiyW9REEE4ty"
      },
      "outputs": [],
      "source": [
        "openai_model = \"gpt-3.5-turbo\"\n",
        "temperature = 0\n",
        "max_attempts = 5\n",
        "attempts = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sFXvAGQqui3d"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(api_key=api_key)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YkKHwQ5hE4tx"
      },
      "source": [
        "#### Define your prompt\n",
        "Here you define two messages to the model:  \n",
        "1. System prompt: Priming the model by telling it how you'd like it to behave.  \n",
        " This is a capability that doesn't exist for when you use the ChatGPT interface in a web browser.  \n",
        "1. User prompt.\n",
        "\n",
        "Note:  \n",
        "OpenAI allows more options for a more sophisticated priming.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3mfrh7UYE4tx"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"Your are an insightful assistant. When you are asked a question, you articulate an answer. Only after you have finished answering the question, you carefully review all the words in the prompt and you state the typos that exist in the prompt, finally, you provide corrections to them.\"\n",
        "user_prompt_oai = \"If neuroscience could extract the last thoughts a person had before they dyed, how would the world be diferent?\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VDFtY0fPE4tz"
      },
      "source": [
        "Define the priming message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ShsIkMVeE4tz"
      },
      "outputs": [],
      "source": [
        "# Create message:\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\",\n",
        "                 \"content\": system_prompt})\n",
        "messages.append({\"role\": \"user\",\n",
        "                 \"content\": user_prompt_oai})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ug5QhUa1E4tz"
      },
      "source": [
        "#### Experiment with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZqme68_E4t0",
        "outputId": "eef9865f-b7f5-409e-83e5-a61621ad087c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: If neuroscience could extract the last thoughts a person had before they dyed, how would the world be diferent?\n",
            "\n",
            "gpt-3.5-turbo's Response: \n",
            "If neuroscience could extract the last thoughts a person had before they died, it would have profound implications for various aspects of society. \n",
            "This ability could potentially revolutionize fields such as psychology, criminology, and end-of-life care. \n",
            "Understanding a person's final thoughts could provide valuable insights into their state of mind, emotional well-being, and potentially help unravel mysteries surrounding their death. \n",
            "It could also offer comfort to loved ones by providing a glimpse into the innermost thoughts of the deceased. \n",
            "However, such technology would raise significant ethical concerns regarding privacy, consent, and the potential misuse of this information. \n",
            "Overall, the world would be both fascinated and apprehensive about the implications of this groundbreaking capability.\n",
            "\n",
            "Typos in the prompt:\n",
            "1. \n",
            "\"dyed\" should be \"died\"\n",
            "2. \n",
            "\"diferent\" should be \"different\"\n",
            "\n",
            "Corrections:\n",
            "If neuroscience could extract the last thoughts a person had before they died, how would the world be different?\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=openai_model,\n",
        "            messages=messages,\n",
        "            temperature=temperature)\n",
        "        response_oai = response.choices[0].message.content.strip()\n",
        "        response_oai = re.sub(r'\\. ', r'. \\n', response_oai)\n",
        "        print(f\"Prompt: {user_prompt_oai}\\n\\n{openai_model}'s Response: \\n{response_oai}\")\n",
        "        break\n",
        "    except Exception as output:\n",
        "        attempts += 1\n",
        "        if attempts >= max_attempts:\n",
        "            print(f\"Quitting due to {openai_model} error: {output}\")\n",
        "            break\n",
        "        print(f\"Attempt #{attempts} failed: {output}\")\n",
        "        time.sleep(1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ajtE_AXTE4t0"
      },
      "source": [
        "****"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8r0QrKVv4z0D"
      },
      "source": [
        "## Part 2 - Using Open Source LLMs Locally\n",
        "Here we set up an open source LLM via Hugging Face's hub.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bfdxZJU24p3J"
      },
      "source": [
        "Install:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcvU5auS4DYW",
        "outputId": "ee4a5a98-1be7-4a93-f4d7-90247c6e75df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 245, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 94, in _iter_built_with_inserted\n",
            "    if installed.version >= version:\n",
            "       ^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 375, in version\n",
            "    self._version = self.dist.version\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_internal\\metadata\\importlib\\_dists.py\", line 177, in version\n",
            "    return parse_version(self._dist.version)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 49, in parse\n",
            "    return Version(version)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"f:\\python\\transformers\\Lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 264, in __init__\n",
            "    match = self._regex.search(version)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: expected string or bytes-like object, got 'NoneType'\n"
          ]
        }
      ],
      "source": [
        "%pip -q install --upgrade transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NbaKGrqK4vEk"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Jpb8ql3W4uO_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Bbe-2z4zhy"
      },
      "source": [
        "### Experiment with Microsoft's `DialoGPT-medium`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9rh3TyEhmP"
      },
      "source": [
        "Settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0Ff8IhAKEhvv"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be93bac84206409b979a1a28ca6913f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\python\\transformers\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laven\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63acdd16c4834235af60eb4ac7be88f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88f957fc586e4f61aabd6b75e3408416",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9efb3a578b9d434991209b9a32ec8a9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dafc819f99c44fb96b2d299f0868487",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20daff0d21cc4044aac6f575c1a5b94c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "hf_model = \"microsoft/DialoGPT-medium\"\n",
        "max_length = 1000\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(hf_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UIH64_Lk6CqU"
      },
      "source": [
        "#### Define your prompt\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v-CVoV5T6C0W"
      },
      "outputs": [],
      "source": [
        "user_prompt_hf = \"If dinosaurs were alive today, would they possess a threat to people?\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HIdIkcWFEr5G"
      },
      "source": [
        "#### Experiment with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egxAV51KHzsM",
        "outputId": "1d33121d-62d5-4eae-9c0a-70943b51a483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "*  Note to user: Feel free to ignore the above warning about padding side.\n",
            "\n",
            "\n",
            "Prompt: If dinosaurs were alive today, would they possess a threat to people?\n",
            "\n",
            "microsoft/DialoGPT-medium's Response: \n",
            "I think they would be more afraid of the humans.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n*  Note to user: Feel free to ignore the above warning about padding side.\")\n",
        "\n",
        "user_input_ids = tokenizer.encode(user_prompt_hf + tokenizer.eos_token, return_tensors='pt')\n",
        "response_hf_encoded = model.generate(user_input_ids,\n",
        "                             max_length=max_length,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "response_hf = tokenizer.decode(response_hf_encoded[:, user_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "print(f\"\\n\\nPrompt: {user_prompt_hf}\\n\\n{hf_model}'s Response: \\n{response_hf}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
